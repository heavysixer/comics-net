{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException\n",
    "import random\n",
    "from time import sleep\n",
    "from os import path\n",
    "from typing import Union\n",
    "\n",
    "import comicvision.webscraper as webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_brackets(title: str) -> Union[str, None]:\n",
    "    regex_brackets = re.search(r\"\\[(.*?)\\]\", title)\n",
    "    if regex_brackets is None:\n",
    "        return None\n",
    "    else:\n",
    "        return regex_brackets.group()\n",
    "    \n",
    "\n",
    "def strip_brackets_from_title(title: str) -> str:\n",
    "    brackets = get_brackets(title)\n",
    "    if brackets is None:\n",
    "        if '--' in title:\n",
    "            return title.split('--')[0].strip()\n",
    "        else:\n",
    "            return title\n",
    "    else:\n",
    "        debracketed_title = title.split(brackets)[0].strip()\n",
    "        if '--' in debracketed_title:\n",
    "            return debracketed_title.split('--')[0].strip()\n",
    "        else:\n",
    "            return debracketed_title\n",
    "\n",
    "\n",
    "def check_if_issue_is_reprinting(title: str) -> bool:\n",
    "    is_second_printing = 'Second Printing' in title\n",
    "    is_2nd_printing = '2nd Printing' in title\n",
    "    is_3rd_printing = '3rd Printing' in title\n",
    "    is_4th_printing = '4th Printing' in title\n",
    "    is_5th_printing = '5th Printing' in title\n",
    "    is_6th_printing = '6th Printing' in title\n",
    "    is_7th_printing = '7th Printing' in title\n",
    "    is_8th_printing = '8th Printing' in title\n",
    "    is_9th_printing = '9th Printing' in title\n",
    "    is_10th_printing = '10th Printing' in title\n",
    "    return (is_second_printing | is_2nd_printing | is_3rd_printing | is_4th_printing | is_5th_printing | is_6th_printing | is_7th_printing |\n",
    "            is_8th_printing | is_9th_printing | is_10th_printing)\n",
    "\n",
    "\n",
    "def check_if_issue_is_duplicate(title: str, on_sale_date: str, metadata_path: str) -> bool: \n",
    "    logged_metadata = []\n",
    "\n",
    "    with jsonlines.open(metadata_path, mode='r') as reader:\n",
    "        for item in reader:\n",
    "            logged_metadata.append(item)\n",
    "\n",
    "    df = pd.DataFrame(logged_metadata)\n",
    "\n",
    "    titles = list(df['title'].unique())\n",
    "    title_is_duplicate = reduce(lambda x, y: x | y, \n",
    "                                [strip_brackets_from_title(title) in strip_brackets_from_title(x) for x in titles])\n",
    "    \n",
    "    if title_is_duplicate:\n",
    "        metadata_on_sale_dates = df[df['title'].apply(strip_brackets_from_title) == strip_brackets_from_title(title)]['on_sale_date'].values\n",
    "        on_sale_date_is_duplicate = on_sale_date in metadata_on_sale_dates\n",
    "        is_reprinting = check_if_issue_is_reprinting(title)\n",
    "        is_duplicate_newsstand = check_if_issue_is_duplicate_newsstand(title, df)\n",
    "        return on_sale_date_is_duplicate | is_reprinting | is_duplicate_newsstand\n",
    "    else:\n",
    "        return title_is_duplicate\n",
    "    \n",
    "    \n",
    "def check_if_issue_is_duplicate_newsstand(title, df) -> bool:\n",
    "    is_newsstand = \"Newsstand\" in title\n",
    "    \n",
    "    is_canadian = \"Canadian\" in title\n",
    "    \n",
    "    titles = list(df['title'].unique())\n",
    "    have_a_direct_sale_no_brackets = reduce(lambda x, y: x | y, \n",
    "                                [str(strip_brackets_from_title(title).split['--'][0].strip() + ' -- Direct Sales') in x for x in titles])\n",
    "    \n",
    "    have_a_direct_sale_no_brackets_2 = reduce(lambda x, y: x | y, \n",
    "                                [str(strip_brackets_from_title(title).split['--'][0].strip() + ' -- Direct') in x for x in titles])\n",
    "    \n",
    "    have_a_direct_sale_brackets = reduce(lambda x, y: x | y, \n",
    "                            [str(strip_brackets_from_title(title) + ' [Direct Sales]') in x for x in titles])\n",
    "    \n",
    "    have_a_direct_sale_brackets_2 = reduce(lambda x, y: x | y, \n",
    "                            [str(strip_brackets_from_title(title) + ' [Direct]') in x for x in titles])\n",
    "    \n",
    "    return ((is_newsstand | is_canadian) & (have_a_direct_sale_no_brackets | have_a_direct_sale_no_brackets_2 | have_a_direct_sale_brackets | have_a_direct_sale_brackets_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# title = 'Batgirl #1 -- poopy fart'\n",
    "# on_sale_date = '2000-02-02'\n",
    "# metadata_path = './metadata/covers.jsonl'\n",
    "\n",
    "\n",
    "# strip_brackets_from_title(title)\n",
    "# # check_if_issue_is_duplicate(title, on_sale_date, metadata_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor this method to make it more functional and easier to test\n",
    "# TODO: add tests!\n",
    "\n",
    "def get_all_from_publisher_page(publisher_url: str, page: int):\n",
    "    \"\"\"\n",
    "    Do a thing...\n",
    "    \"\"\"\n",
    "    # global val\n",
    "    URL = 'https://www.comics.org'\n",
    "\n",
    "    # get publisher page\n",
    "    publisher_html = webscraper.simple_get(publisher_url + '?page={}'.format(page))\n",
    "    publisher_soup = webscraper.transform_simple_get_html(publisher_html)\n",
    "    \n",
    "    # parse series table from publisher page\n",
    "    series_name = [result.find('a').contents[0] for result in publisher_soup.find_all('td', {'class': 'name'})]\n",
    "    series_href = [result.find('a')['href'] for result in  publisher_soup.find_all('td', {'class': 'name'})]\n",
    "    series_year = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'year'})]\n",
    "    series_issue_count = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'issue_count'})]\n",
    "    series_published = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'published'})]\n",
    "    \n",
    "    # create dataframe of publisher series (on page)\n",
    "    series_df = pd.DataFrame(list(zip(series_name, series_href, series_year, series_issue_count, series_published)),\n",
    "                             columns=['name', 'href', 'year', 'issue_count', 'published'])\n",
    "\n",
    "    # parse issue count as int from issue_count column\n",
    "    series_df['issue_count_int'] = series_df['issue_count'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "    # iterate over series dataframe and get issue covers and metadata\n",
    "    for series_name, series_page_href, issue_count in zip(series_df['name'], series_df['href'], series_df['issue_count_int']):\n",
    "        \n",
    "        if issue_count < 12:\n",
    "            pass\n",
    "        else:\n",
    "            print(series_name, series_page_href, issue_count)\n",
    "            \n",
    "            # construct series page url\n",
    "            series_page_url  = URL + series_page_href\n",
    "\n",
    "            # get series page\n",
    "            series_page_html = webscraper.simple_get(series_page_url)\n",
    "            series_page_soup = webscraper.transform_simple_get_html(series_page_html)\n",
    "\n",
    "            # get cover gallery url for series\n",
    "            if series_page_soup.find('a',  href=True, text='Cover Gallery') is None:\n",
    "                pass\n",
    "            else:\n",
    "                cover_gallery_href = series_page_soup.find('a',  href=True, text='Cover Gallery')['href']\n",
    "                cover_gallery_base_url = URL + cover_gallery_href\n",
    "\n",
    "                # get cover gallery page\n",
    "                cover_gallery_html = webscraper.simple_get(cover_gallery_base_url)\n",
    "                cover_gallery_soup = webscraper.transform_simple_get_html(cover_gallery_html)\n",
    "\n",
    "                if len(cover_gallery_soup.find_all('a', {'class': \"btn btn-default btn-sm\"})) == 0:\n",
    "\n",
    "                    # get issue hrefs from all linked issues on cover gallery\n",
    "                    cover_gallery_hrefs = filter(lambda x: '/issue/' in x['href'] and '/cover/' not in x['href'], cover_gallery_soup.find_all('a',  href=True))\n",
    "                    issue_hrefs = [x['href'] for x in cover_gallery_hrefs]\n",
    "\n",
    "                    #  construct issue urls from issue hrefs\n",
    "                    issue_urls = [URL + issue_href for issue_href in  issue_hrefs]\n",
    "\n",
    "                    # scrape issues\n",
    "                    for issue_url in issue_urls:\n",
    "\n",
    "                        # get issue page\n",
    "                        issue_html = webscraper.simple_get(issue_url)\n",
    "                        issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                        # metadata\n",
    "                        metadata = {}\n",
    "                        metadata['series_name'] = series_name.replace('/', '|')\n",
    "\n",
    "                        # scrape metadata from issue page\n",
    "                        # title, price, pages, color, dimension, paper_stock, binding, publishing_format\n",
    "                        def get_issue_metadata(soup, name):\n",
    "                            if len(soup.find_all('dd', id=name)) > 0:\n",
    "                                if (name != 'issue_indicia_publisher') & (name != 'issue_brand'):\n",
    "                                    return soup.find_all('dd', id=name)[0].contents[0].strip()\n",
    "                                else:\n",
    "                                    try:\n",
    "                                        return soup.find_all('dd', id=name)[0].find('a').contents[0]\n",
    "                                    except:\n",
    "                                        return \"\"\n",
    "                            else:\n",
    "                                return \"\"\n",
    "\n",
    "                        # post process the issue title removing extraneous characters\n",
    "                        metadata['title'] = issue_soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1].replace('/', '|')\n",
    "                        metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "\n",
    "                        # check if issue is redundant to an issue  already we pulled (variant)\n",
    "                        if path.exists('./metadata/covers.jsonl'):\n",
    "                            is_duplicate = check_if_issue_is_duplicate(title=metadata['title'], \n",
    "                                                                       on_sale_date=metadata['on_sale_date'], \n",
    "                                                                       metadata_path='./metadata/covers.jsonl')\n",
    "\n",
    "                            if is_duplicate:\n",
    "                                pass\n",
    "                            else:\n",
    "                                metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "                                metadata['indicia_frequency'] = get_issue_metadata(issue_soup, name='indicia_frequency')\n",
    "                                metadata['issue_indicia_publisher'] = get_issue_metadata(issue_soup, name='issue_indicia_publisher')\n",
    "                                metadata['issue_brand'] = get_issue_metadata(issue_soup, name='issue_brand')\n",
    "                                metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "                                metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "                                metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "                                metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "                                metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "                                metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "                                metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "                                metadata['rating'] = get_issue_metadata(issue_soup, name='rating')\n",
    "                                metadata['indexer_notes'] = \" | \".join([x.contents[0].replace('\\n', '').strip() for x in issue_soup.find_all('p')])\n",
    "\n",
    "                                all_issue_credits = list(zip(\n",
    "                                    issue_soup.find_all('span', {'class': 'credit_label'}),  \n",
    "                                    issue_soup.find_all('span', {'class': 'credit_value'})))\n",
    "\n",
    "                                metadata['synopsis'] = \" | \".join(list(filter(lambda x: x != '', [x[1].contents[0] if x[0].contents[0] == 'Synopsis' else '' for x in all_issue_credits])))\n",
    "\n",
    "                                # get cover section\n",
    "                                cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "                                cover_credits = list(zip(\n",
    "                                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                ))\n",
    "\n",
    "                                metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "                                metadata.pop('cover_reprints', None)\n",
    "\n",
    "                                # get the cover url\n",
    "                                cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "                                cover_img_url = URL + cover_img_href\n",
    "\n",
    "                                # get cover page\n",
    "                                cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "                                cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "                                # get image urls from cover page\n",
    "                                cover_img_soup.find_all('img')\n",
    "\n",
    "                                cover_divs = cover_img_soup.find_all('div', {'class': 'issue_covers'})[0].find_all('div')\n",
    "\n",
    "                                def get_variant_cover_name(cover_name: str):\n",
    "                                    if get_brackets(cover_name) is None:\n",
    "                                        return 'Original Cover'\n",
    "                                    else:\n",
    "                                        return get_brackets(cover_name).replace('[','').replace(']','')\n",
    "\n",
    "                                # go into variant url and pull metadata\n",
    "                                cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "                                cover_name = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "                                cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "                                covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "                                covers_dict = {}\n",
    "                                for cover in covers:\n",
    "                                    name = cover[0]\n",
    "                                    url = cover[1]\n",
    "                                    image = cover[2]\n",
    "\n",
    "                                    covers_dict[name] = {}\n",
    "                                    covers_dict[name]['cover_url'] = url\n",
    "                                    covers_dict[name]['image_url'] = image\n",
    "\n",
    "                                metadata['variant_covers'] = {}\n",
    "\n",
    "                                for variant_name in covers_dict:\n",
    "                                    if 'Second Printing' in variant_name:\n",
    "                                        pass\n",
    "                                    elif ('Newsstand' in variant_name) & ('Direct Sales' in covers_dict.keys()):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "                                        # get issue page\n",
    "                                        issue_html = webscraper.simple_get(issue_url)\n",
    "                                        issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                                        cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                        cover_credits = list(zip(\n",
    "                                                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                                        ))\n",
    "\n",
    "                                        cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "                                        cover_credits.pop('cover_reprints', None)\n",
    "\n",
    "                                        save_as = \"{} -- {} -- {} -- {}\".format(metadata['series_name'], strip_brackets_from_title(metadata['title']), variant_name, metadata['on_sale_date'], )\n",
    "                                        save_to = './covers/' + save_as + '.jpg'\n",
    "\n",
    "                                        cover_credits['cover_image_file_name'] = save_as\n",
    "\n",
    "                                        metadata['variant_covers'][variant_name] = cover_credits\n",
    "\n",
    "                                        # save cover image\n",
    "                                        urllib.request.urlretrieve(covers_dict[variant_name]['image_url'], save_to)\n",
    "\n",
    "\n",
    "                                # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "                                # save metadata\n",
    "                                with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "                                    writer.write(metadata)\n",
    "\n",
    "                                # TODO: write to log... timestamp/publisher/series/issue/\n",
    "                                now = datetime.datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                                publisher_int = publisher_url.split('/')[-2]\n",
    "\n",
    "                                log = {'timestamp': now, 'publisher': publisher_int, 'series': metadata['series_name'],  'issue': metadata['title']}\n",
    "                                with jsonlines.open('./metadata/log.jsonl', mode='a') as writer:\n",
    "                                    writer.write(log)\n",
    "\n",
    "                                # slow down the requests so we don't take too many resources and get blocked\n",
    "                                sleep(random.uniform(5, 10))\n",
    "\n",
    "\n",
    "                        else:\n",
    "                            metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "                            metadata['indicia_frequency'] = get_issue_metadata(issue_soup, name='indicia_frequency')\n",
    "                            metadata['issue_indicia_publisher'] = get_issue_metadata(issue_soup, name='issue_indicia_publisher')\n",
    "                            metadata['issue_brand'] = get_issue_metadata(issue_soup, name='issue_brand')\n",
    "                            metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "                            metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "                            metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "                            metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "                            metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "                            metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "                            metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "                            metadata['rating'] = get_issue_metadata(issue_soup, name='rating')\n",
    "                            metadata['indexer_notes'] = \" | \".join([x.contents[0].replace('\\n', '').strip() for x in issue_soup.find_all('p')])\n",
    "\n",
    "                            all_issue_credits = list(zip(\n",
    "                                issue_soup.find_all('span', {'class': 'credit_label'}), \n",
    "                                issue_soup.find_all('span', {'class': 'credit_value'})))\n",
    "\n",
    "                            metadata['synopsis'] = \" | \".join(list(filter(lambda x: x != '', [x[1].contents[0] if x[0].contents[0] == 'Synopsis' else '' for x in all_issue_credits])))\n",
    "\n",
    "                            # get cover section\n",
    "                            cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                            # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "                            cover_credits = list(zip(\n",
    "                                [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                            ))\n",
    "\n",
    "                            metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "                            metadata.pop('cover_reprints', None)\n",
    "\n",
    "                            # get the cover url\n",
    "                            cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "                            cover_img_url = URL + cover_img_href\n",
    "\n",
    "                            # get cover page\n",
    "                            cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "                            cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "                            # get image urls from cover page\n",
    "                            cover_img_soup.find_all('img')\n",
    "\n",
    "                            cover_divs = cover_img_soup.find_all('div', {'class': 'issue_covers'})[0].find_all('div')\n",
    "\n",
    "                            def get_variant_cover_name(cover_name: str):\n",
    "                                if get_brackets(cover_name) is None:\n",
    "                                    return 'Original Cover'\n",
    "                                else:\n",
    "                                    return get_brackets(cover_name).replace('[','').replace(']','')\n",
    "\n",
    "                            # go into variant url and pull metadata\n",
    "                            cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "                            cover_name = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "                            cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "                            covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "                            covers_dict = {}\n",
    "                            for cover in covers:\n",
    "                                name = cover[0]\n",
    "                                url = cover[1]\n",
    "                                image = cover[2]\n",
    "\n",
    "                                covers_dict[name] = {}\n",
    "                                covers_dict[name]['cover_url'] = url\n",
    "                                covers_dict[name]['image_url'] = image\n",
    "\n",
    "                            metadata['variant_covers'] = {}\n",
    "\n",
    "                            for variant_name in covers_dict:\n",
    "                                if 'Second Printing' in variant_name:\n",
    "                                    pass\n",
    "                                elif ('Newsstand' in variant_name) & ('Direct Sales' in covers_dict.keys()):\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "                                    # get issue page\n",
    "                                    issue_html = webscraper.simple_get(issue_url)\n",
    "                                    issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                                    cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                    cover_credits = list(zip(\n",
    "                                                        [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                                        [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                                    ))\n",
    "\n",
    "                                    cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "                                    cover_credits.pop('cover_reprints', None)\n",
    "\n",
    "                                    save_as = \"{} -- {} -- {} -- {}\".format(metadata['series_name'], strip_brackets_from_title(metadata['title']), variant_name, metadata['on_sale_date'], )\n",
    "                                    save_to = './covers/' + save_as + '.jpg'\n",
    "\n",
    "                                    cover_credits['cover_image_file_name'] = save_as\n",
    "\n",
    "                                    metadata['variant_covers'][variant_name] = cover_credits\n",
    "\n",
    "                                    # save cover image\n",
    "                                    urllib.request.urlretrieve(covers_dict[variant_name]['image_url'], save_to)\n",
    "\n",
    "\n",
    "                            # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "                            # save metadata\n",
    "                            with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "                                writer.write(metadata)\n",
    "\n",
    "                            # TODO: write to log... timestamp/publisher/series/issue/\n",
    "                            now = datetime.datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                            publisher_int = publisher_url.split('/')[-2]\n",
    "\n",
    "                            log = {'timestamp': now, 'publisher': publisher_int, 'series': metadata['series_name'],  'issue': metadata['title']}\n",
    "                            with jsonlines.open('./metadata/log.jsonl', mode='a') as writer:\n",
    "                                writer.write(log)\n",
    "\n",
    "                            # slow down the requests so we don't take too many resources and get blocked\n",
    "                            sleep(random.uniform(5, 10))\n",
    "\n",
    "                else:\n",
    "                    cover_gallery_pages = list(filter(lambda x: x.isdigit(), [x.contents[0] for x in cover_gallery_soup.find_all('a', {'class': \"btn btn-default btn-sm\"})]))\n",
    "                    cover_gallery_range = max([int(x) for x in cover_gallery_pages])\n",
    "\n",
    "                    for i in range(1, cover_gallery_range + 1):\n",
    "                        cover_gallery_url = str(cover_gallery_base_url + '/?page={}').format(i)\n",
    "                        cover_gallery_html = webscraper.simple_get(cover_gallery_url)\n",
    "                        cover_gallery_soup = webscraper.transform_simple_get_html(cover_gallery_html)\n",
    "\n",
    "                        # get issue hrefs from all linked issues on cover gallery\n",
    "                        cover_gallery_hrefs = filter(lambda x: '/issue/' in x['href'] and '/cover/' not in x['href'], cover_gallery_soup.find_all('a',  href=True))\n",
    "                        issue_hrefs = [x['href'] for x in cover_gallery_hrefs]\n",
    "\n",
    "                        #  construct issue urls from issue hrefs\n",
    "                        issue_urls = [URL + issue_href for issue_href in  issue_hrefs]\n",
    "\n",
    "                        # scrape issues\n",
    "                        for issue_url in issue_urls:\n",
    "\n",
    "                            # get issue page\n",
    "                            issue_html = webscraper.simple_get(issue_url)\n",
    "                            issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                            # metadata\n",
    "                            metadata = {}\n",
    "                            metadata['series_name'] = series_name.replace('/', '|')\n",
    "\n",
    "                            # scrape metadata from issue page\n",
    "                            # title, price, pages, color, dimension, paper_stock, binding, publishing_format\n",
    "                            def get_issue_metadata(soup, name):\n",
    "                                if len(soup.find_all('dd', id=name)) > 0:\n",
    "                                    if (name != 'issue_indicia_publisher') & (name != 'issue_brand'):\n",
    "                                        return soup.find_all('dd', id=name)[0].contents[0].strip()\n",
    "                                    else:\n",
    "                                        try:\n",
    "                                            return soup.find_all('dd', id=name)[0].find('a').contents[0]\n",
    "                                        except:\n",
    "                                            return \"\"\n",
    "                                else:\n",
    "                                    return \"\"\n",
    "\n",
    "                            metadata['title'] = issue_soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1].replace('/', '|')\n",
    "                            metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "\n",
    "                            # check if issue is redundant to an issue  already we pulled (variant)\n",
    "                            if path.exists('./metadata/covers.jsonl'):\n",
    "                                is_duplicate = check_if_issue_is_duplicate(title=metadata['title'], \n",
    "                                                                           on_sale_date=metadata['on_sale_date'], \n",
    "                                                                           metadata_path='./metadata/covers.jsonl')\n",
    "\n",
    "                                if is_duplicate:\n",
    "                                    pass\n",
    "                                else:\n",
    "                                    metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "                                    metadata['indicia_frequency'] = get_issue_metadata(issue_soup, name='indicia_frequency')\n",
    "                                    metadata['issue_indicia_publisher'] = get_issue_metadata(issue_soup, name='issue_indicia_publisher')\n",
    "                                    metadata['issue_brand'] = get_issue_metadata(issue_soup, name='issue_brand')\n",
    "                                    metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "                                    metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "                                    metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "                                    metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "                                    metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "                                    metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "                                    metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "                                    metadata['rating'] = get_issue_metadata(issue_soup, name='rating')\n",
    "                                    metadata['indexer_notes'] = \" | \".join([x.contents[0].replace('\\n', '').strip() for x in issue_soup.find_all('p')])\n",
    "\n",
    "                                    all_issue_credits = list(zip(\n",
    "                                        issue_soup.find_all('span', {'class': 'credit_label'}),  \n",
    "                                        issue_soup.find_all('span', {'class': 'credit_value'})))\n",
    "\n",
    "                                    metadata['synopsis'] = \" | \".join(list(filter(lambda x: x != '', [x[1].contents[0] if x[0].contents[0] == 'Synopsis' else '' for x in all_issue_credits])))\n",
    "\n",
    "                                    # get cover section\n",
    "                                    cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                    # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "                                    cover_credits = list(zip(\n",
    "                                        [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                        [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                    ))\n",
    "\n",
    "                                    metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "                                    metadata.pop('cover_reprints', None)\n",
    "\n",
    "                                    # get the cover url\n",
    "                                    cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "                                    cover_img_url = URL + cover_img_href\n",
    "\n",
    "                                    # get cover page\n",
    "                                    cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "                                    cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "                                    # get image urls from cover page\n",
    "                                    cover_img_soup.find_all('img')\n",
    "\n",
    "                                    cover_divs = cover_img_soup.find_all('div', {'class': 'issue_covers'})[0].find_all('div')\n",
    "\n",
    "                                    def get_variant_cover_name(cover_name: str):\n",
    "                                        if get_brackets(cover_name) is None:\n",
    "                                            return 'Original Cover'\n",
    "                                        else:\n",
    "                                            return get_brackets(cover_name).replace('[','').replace(']','')\n",
    "\n",
    "                                    # go into variant url and pull metadata\n",
    "                                    cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "                                    cover_name = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "                                    cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "                                    covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "                                    covers_dict = {}\n",
    "                                    for cover in covers:\n",
    "                                        name = cover[0]\n",
    "                                        url = cover[1]\n",
    "                                        image = cover[2]\n",
    "\n",
    "                                        covers_dict[name] = {}\n",
    "                                        covers_dict[name]['cover_url'] = url\n",
    "                                        covers_dict[name]['image_url'] = image\n",
    "\n",
    "                                    metadata['variant_covers'] = {}\n",
    "\n",
    "                                    for variant_name in covers_dict:\n",
    "                                        if 'Second Printing' in variant_name:\n",
    "                                            pass\n",
    "                                        elif ('Newsstand' in variant_name) & ('Direct Sales' in covers_dict.keys()):\n",
    "                                            pass\n",
    "                                        else:\n",
    "                                            issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "                                            # get issue page\n",
    "                                            issue_html = webscraper.simple_get(issue_url)\n",
    "                                            issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                                            cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                            cover_credits = list(zip(\n",
    "                                                                [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                                                [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                                            ))\n",
    "\n",
    "                                            cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "                                            cover_credits.pop('cover_reprints', None)\n",
    "\n",
    "                                            save_as = \"{} -- {} -- {} -- {}\".format(metadata['series_name'], strip_brackets_from_title(metadata['title']), variant_name, metadata['on_sale_date'], )\n",
    "                                            save_to = './covers/' + save_as + '.jpg'\n",
    "\n",
    "                                            cover_credits['cover_image_file_name'] = save_as\n",
    "\n",
    "                                            metadata['variant_covers'][variant_name] = cover_credits\n",
    "\n",
    "                                            # save cover image\n",
    "                                            urllib.request.urlretrieve(covers_dict[variant_name]['image_url'], save_to)\n",
    "\n",
    "\n",
    "                                    # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "                                    # save metadata\n",
    "                                    with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "                                        writer.write(metadata)\n",
    "\n",
    "                                    # TODO: write to log... timestamp/publisher/series/issue/\n",
    "                                    now = datetime.datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                                    publisher_int = publisher_url.split('/')[-2]\n",
    "\n",
    "                                    log = {'timestamp': now, 'publisher': publisher_int, 'series': metadata['series_name'],  'issue': metadata['title']}\n",
    "                                    with jsonlines.open('./metadata/log.jsonl', mode='a') as writer:\n",
    "                                        writer.write(log)\n",
    "\n",
    "                                    # slow down the requests so we don't take too many resources and get blocked\n",
    "                                    sleep(random.uniform(5, 10))\n",
    "\n",
    "\n",
    "                            else:\n",
    "                                metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "                                metadata['indicia_frequency'] = get_issue_metadata(issue_soup, name='indicia_frequency')\n",
    "                                metadata['issue_indicia_publisher'] = get_issue_metadata(issue_soup, name='issue_indicia_publisher')\n",
    "                                metadata['issue_brand'] = get_issue_metadata(issue_soup, name='issue_brand')\n",
    "                                metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "                                metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "                                metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "                                metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "                                metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "                                metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "                                metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "                                metadata['rating'] = get_issue_metadata(issue_soup, name='rating')\n",
    "                                metadata['indexer_notes'] = \" | \".join([x.contents[0].replace('\\n', '').strip() for x in issue_soup.find_all('p')])\n",
    "\n",
    "                                all_issue_credits = list(zip(\n",
    "                                    issue_soup.find_all('span', {'class': 'credit_label'}), \n",
    "                                    issue_soup.find_all('span', {'class': 'credit_value'})))\n",
    "\n",
    "                                metadata['synopsis'] = \" | \".join(list(filter(lambda x: x != '', [x[1].contents[0] if x[0].contents[0] == 'Synopsis' else '' for x in all_issue_credits])))\n",
    "\n",
    "                                # get cover section\n",
    "                                cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "                                cover_credits = list(zip(\n",
    "                                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                ))\n",
    "\n",
    "                                metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "                                metadata.pop('cover_reprints', None)\n",
    "\n",
    "                                # get the cover url\n",
    "                                cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "                                cover_img_url = URL + cover_img_href\n",
    "\n",
    "                                # get cover page\n",
    "                                cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "                                cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "                                # get image urls from cover page\n",
    "                                cover_img_soup.find_all('img')\n",
    "\n",
    "                                cover_divs = cover_img_soup.find_all('div', {'class': 'issue_covers'})[0].find_all('div')\n",
    "\n",
    "                                def get_variant_cover_name(cover_name: str):\n",
    "                                    if get_brackets(cover_name) is None:\n",
    "                                        return 'Original Cover'\n",
    "                                    else:\n",
    "                                        return get_brackets(cover_name).replace('[','').replace(']','')\n",
    "\n",
    "                                # go into variant url and pull metadata\n",
    "                                cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "                                cover_name = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "                                cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "                                covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "                                covers_dict = {}\n",
    "                                for cover in covers:\n",
    "                                    name = cover[0]\n",
    "                                    url = cover[1]\n",
    "                                    image = cover[2]\n",
    "\n",
    "                                    covers_dict[name] = {}\n",
    "                                    covers_dict[name]['cover_url'] = url\n",
    "                                    covers_dict[name]['image_url'] = image\n",
    "\n",
    "                                metadata['variant_covers'] = {}\n",
    "\n",
    "                                for variant_name in covers_dict:\n",
    "                                    if 'Second Printing' in variant_name:\n",
    "                                        pass\n",
    "                                    elif ('Newsstand' in variant_name) & ('Direct Sales' in covers_dict.keys()):\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "                                        # get issue page\n",
    "                                        issue_html = webscraper.simple_get(issue_url)\n",
    "                                        issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                                        cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                                        cover_credits = list(zip(\n",
    "                                                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                                                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                                                        ))\n",
    "\n",
    "                                        cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "                                        cover_credits.pop('cover_reprints', None)\n",
    "\n",
    "                                        save_as = \"{} -- {} -- {} -- {}\".format(metadata['series_name'], strip_brackets_from_title(metadata['title']), variant_name, metadata['on_sale_date'], )\n",
    "                                        save_to = './covers/' + save_as + '.jpg'\n",
    "\n",
    "                                        cover_credits['cover_image_file_name'] = save_as\n",
    "\n",
    "                                        metadata['variant_covers'][variant_name] = cover_credits\n",
    "\n",
    "                                        # save cover image\n",
    "                                        urllib.request.urlretrieve(covers_dict[variant_name]['image_url'], save_to)\n",
    "\n",
    "\n",
    "                                # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "                                # save metadata\n",
    "                                with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "                                    writer.write(metadata)\n",
    "\n",
    "                                # TODO: write to log... timestamp/publisher/series/issue/\n",
    "                                now = datetime.datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "                                publisher_int = publisher_url.split('/')[-2]\n",
    "\n",
    "                                log = {'timestamp': now, 'publisher': publisher_int, 'series': metadata['series_name'],  'issue': metadata['title']}\n",
    "                                with jsonlines.open('./metadata/log.jsonl', mode='a') as writer:\n",
    "                                    writer.write(log)\n",
    "\n",
    "                                # slow down the requests so we don't take too many resources and get blocked\n",
    "                                sleep(random.uniform(5, 10))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  ran page publisher/54/ page=1,2, 6\n",
    "\n",
    "# get_all_from_publisher_page(publisher_url='https://www.comics.org/publisher/54/', page=26)\n",
    "\n",
    "# TODO: finish running page 26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: load metadata and return aggregate / summary statistics\n",
    "# # TODO: write method to display cover image w/ cover metadata and add annotations to image\n",
    "# # TODO: consider ideate/innotater for annotating directly in Jupyter notebooks\n",
    "\n",
    "logged_metadata = []\n",
    "\n",
    "with jsonlines.open('./metadata/covers.jsonl', mode='r') as reader:\n",
    "    for item in reader:\n",
    "        logged_metadata.append(item)\n",
    "        \n",
    "df = pd.DataFrame(logged_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cover_characters</th>\n",
       "      <td>4268</td>\n",
       "      <td>2083</td>\n",
       "      <td>Superman</td>\n",
       "      <td>266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_colors</th>\n",
       "      <td>4718</td>\n",
       "      <td>292</td>\n",
       "      <td>?</td>\n",
       "      <td>2694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_editing</th>\n",
       "      <td>90</td>\n",
       "      <td>5</td>\n",
       "      <td>Curtis King</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_first line of dialogue or text</th>\n",
       "      <td>428</td>\n",
       "      <td>347</td>\n",
       "      <td>Ta-ta, Bats! Looks like this will be our final...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_genre</th>\n",
       "      <td>4683</td>\n",
       "      <td>30</td>\n",
       "      <td>superhero</td>\n",
       "      <td>3837</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_inks</th>\n",
       "      <td>4733</td>\n",
       "      <td>702</td>\n",
       "      <td>Stan Kaye</td>\n",
       "      <td>253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_job number</th>\n",
       "      <td>635</td>\n",
       "      <td>461</td>\n",
       "      <td>C-422</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_keywords</th>\n",
       "      <td>1153</td>\n",
       "      <td>792</td>\n",
       "      <td>celebrity</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_letters</th>\n",
       "      <td>3978</td>\n",
       "      <td>51</td>\n",
       "      <td>?</td>\n",
       "      <td>2707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_pencils</th>\n",
       "      <td>4735</td>\n",
       "      <td>689</td>\n",
       "      <td>Curt Swan</td>\n",
       "      <td>373</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_script</th>\n",
       "      <td>662</td>\n",
       "      <td>19</td>\n",
       "      <td>?</td>\n",
       "      <td>633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cover_synopsis</th>\n",
       "      <td>34</td>\n",
       "      <td>31</td>\n",
       "      <td>Grockk and Firegirl battle Sulphur and Brimest...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_binding</th>\n",
       "      <td>4822</td>\n",
       "      <td>9</td>\n",
       "      <td>Saddle-stitched</td>\n",
       "      <td>2119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_color</th>\n",
       "      <td>4822</td>\n",
       "      <td>2</td>\n",
       "      <td>Color</td>\n",
       "      <td>3005</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_dimensions</th>\n",
       "      <td>4822</td>\n",
       "      <td>18</td>\n",
       "      <td>standard Golden Age US; then standard Silver A...</td>\n",
       "      <td>1015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_paper_stock</th>\n",
       "      <td>4822</td>\n",
       "      <td>10</td>\n",
       "      <td></td>\n",
       "      <td>2826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>format_publishing_format</th>\n",
       "      <td>4822</td>\n",
       "      <td>13</td>\n",
       "      <td>Was Ongoing Series</td>\n",
       "      <td>2516</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indexer_notes</th>\n",
       "      <td>4822</td>\n",
       "      <td>3677</td>\n",
       "      <td>This issue has variants. |</td>\n",
       "      <td>359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>indicia_frequency</th>\n",
       "      <td>4822</td>\n",
       "      <td>76</td>\n",
       "      <td>monthly</td>\n",
       "      <td>3276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_brand</th>\n",
       "      <td>4822</td>\n",
       "      <td>27</td>\n",
       "      <td>DC [bullet]</td>\n",
       "      <td>1537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_indicia_publisher</th>\n",
       "      <td>4822</td>\n",
       "      <td>12</td>\n",
       "      <td>DC Comics</td>\n",
       "      <td>1581</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_pages</th>\n",
       "      <td>4822</td>\n",
       "      <td>34</td>\n",
       "      <td>36</td>\n",
       "      <td>3207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>issue_price</th>\n",
       "      <td>4822</td>\n",
       "      <td>149</td>\n",
       "      <td>0.10 USD</td>\n",
       "      <td>1426</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>on_sale_date</th>\n",
       "      <td>4822</td>\n",
       "      <td>3318</td>\n",
       "      <td></td>\n",
       "      <td>280</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rating</th>\n",
       "      <td>4822</td>\n",
       "      <td>11</td>\n",
       "      <td>Approved by the Comics Code Authority</td>\n",
       "      <td>2873</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>series_name</th>\n",
       "      <td>4822</td>\n",
       "      <td>36</td>\n",
       "      <td>Action Comics</td>\n",
       "      <td>1136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>synopsis</th>\n",
       "      <td>4822</td>\n",
       "      <td>2980</td>\n",
       "      <td></td>\n",
       "      <td>1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>title</th>\n",
       "      <td>4822</td>\n",
       "      <td>4625</td>\n",
       "      <td>Batgirl #24</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     count unique  \\\n",
       "cover_characters                      4268   2083   \n",
       "cover_colors                          4718    292   \n",
       "cover_editing                           90      5   \n",
       "cover_first line of dialogue or text   428    347   \n",
       "cover_genre                           4683     30   \n",
       "cover_inks                            4733    702   \n",
       "cover_job number                       635    461   \n",
       "cover_keywords                        1153    792   \n",
       "cover_letters                         3978     51   \n",
       "cover_pencils                         4735    689   \n",
       "cover_script                           662     19   \n",
       "cover_synopsis                          34     31   \n",
       "format_binding                        4822      9   \n",
       "format_color                          4822      2   \n",
       "format_dimensions                     4822     18   \n",
       "format_paper_stock                    4822     10   \n",
       "format_publishing_format              4822     13   \n",
       "indexer_notes                         4822   3677   \n",
       "indicia_frequency                     4822     76   \n",
       "issue_brand                           4822     27   \n",
       "issue_indicia_publisher               4822     12   \n",
       "issue_pages                           4822     34   \n",
       "issue_price                           4822    149   \n",
       "on_sale_date                          4822   3318   \n",
       "rating                                4822     11   \n",
       "series_name                           4822     36   \n",
       "synopsis                              4822   2980   \n",
       "title                                 4822   4625   \n",
       "\n",
       "                                                                                    top  \\\n",
       "cover_characters                                                               Superman   \n",
       "cover_colors                                                                          ?   \n",
       "cover_editing                                                               Curtis King   \n",
       "cover_first line of dialogue or text  Ta-ta, Bats! Looks like this will be our final...   \n",
       "cover_genre                                                                   superhero   \n",
       "cover_inks                                                                    Stan Kaye   \n",
       "cover_job number                                                                  C-422   \n",
       "cover_keywords                                                                celebrity   \n",
       "cover_letters                                                                         ?   \n",
       "cover_pencils                                                                 Curt Swan   \n",
       "cover_script                                                                          ?   \n",
       "cover_synopsis                        Grockk and Firegirl battle Sulphur and Brimest...   \n",
       "format_binding                                                          Saddle-stitched   \n",
       "format_color                                                                      Color   \n",
       "format_dimensions                     standard Golden Age US; then standard Silver A...   \n",
       "format_paper_stock                                                                        \n",
       "format_publishing_format                                             Was Ongoing Series   \n",
       "indexer_notes                                               This issue has variants. |    \n",
       "indicia_frequency                                                               monthly   \n",
       "issue_brand                                                                 DC [bullet]   \n",
       "issue_indicia_publisher                                                       DC Comics   \n",
       "issue_pages                                                                          36   \n",
       "issue_price                                                                    0.10 USD   \n",
       "on_sale_date                                                                              \n",
       "rating                                            Approved by the Comics Code Authority   \n",
       "series_name                                                               Action Comics   \n",
       "synopsis                                                                                  \n",
       "title                                                                       Batgirl #24   \n",
       "\n",
       "                                      freq  \n",
       "cover_characters                       266  \n",
       "cover_colors                          2694  \n",
       "cover_editing                           42  \n",
       "cover_first line of dialogue or text     3  \n",
       "cover_genre                           3837  \n",
       "cover_inks                             253  \n",
       "cover_job number                         4  \n",
       "cover_keywords                          27  \n",
       "cover_letters                         2707  \n",
       "cover_pencils                          373  \n",
       "cover_script                           633  \n",
       "cover_synopsis                           2  \n",
       "format_binding                        2119  \n",
       "format_color                          3005  \n",
       "format_dimensions                     1015  \n",
       "format_paper_stock                    2826  \n",
       "format_publishing_format              2516  \n",
       "indexer_notes                          359  \n",
       "indicia_frequency                     3276  \n",
       "issue_brand                           1537  \n",
       "issue_indicia_publisher               1581  \n",
       "issue_pages                           3207  \n",
       "issue_price                           1426  \n",
       "on_sale_date                           280  \n",
       "rating                                2873  \n",
       "series_name                           1136  \n",
       "synopsis                              1494  \n",
       "title                                    4  "
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def get_issue_number_from_title(title):\n",
    "    issue = re.search(r\"([#?])(\\d+)\\b\", title.replace(',', ''))\n",
    "    if issue is None:\n",
    "        return np.nan\n",
    "    else:\n",
    "        return np.int(issue.group().replace('#', ''))\n",
    "\n",
    "df['issue_number'] = df['title'].apply(get_issue_number_from_title)\n",
    "\n",
    "df.drop(['issue_number', 'variant_covers'], axis=1).describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Batman [Bruce Wayne]        928\n",
       "Superman                    757\n",
       "Robin [Dick Grayson]        464\n",
       "Kal-El]                     328\n",
       "Superman [Clark Kent        320\n",
       "Batman                      318\n",
       "Superman [Clark Kent]       204\n",
       "Lois Lane                   167\n",
       "Superboy                    125\n",
       "Bob Hope                    102\n",
       "Lex Luthor                   97\n",
       "Batgirl [Barbara Gordon]     81\n",
       "Joker                        78\n",
       "Supergirl                    77\n",
       "Batgirl [Cassandra Cain]     73\n",
       "Jimmy Olsen                  63\n",
       "Jerry Lewis                  60\n",
       "Superboy [Clark Kent]        52\n",
       "Doiby Dickles                49\n",
       "Robin                        47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get list of unique characters across all  covers\n",
    "def get_value_counts(df, column):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    return pd.Series(\"; \".join(df[column].dropna()).split('; ')).value_counts()\n",
    "\n",
    "get_value_counts(df, 'cover_characters')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Curt Swan                384\n",
       "Win Mortimer             210\n",
       "Sheldon Moldoff          137\n",
       "Bob Oksner               113\n",
       "Gil Kane                 103\n",
       "Dave Johnson (signed)     99\n",
       "Ross Andru (signed)       86\n",
       "Owen Fitzgerald           78\n",
       "Jim Aparo (signed)        76\n",
       "Wayne Boring              68\n",
       "Neal Adams                65\n",
       "Dick Sprang               59\n",
       "Jerry Grandenetti         59\n",
       "Jack Burnley              58\n",
       "Kelley Jones (signed)     51\n",
       "Ed Hannigan (signed)      50\n",
       "Al Plastino               49\n",
       "Jerry Ordway (signed)     49\n",
       "J. G. Jones (signed)      49\n",
       "Tom Grummett (signed)     47\n",
       "dtype: int64"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_value_counts(df, 'cover_pencils')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Action Comics                                  1136\n",
       "Batman                                         1089\n",
       "Adventure Comics                                528\n",
       "Detective Comics                                330\n",
       "Adventures of Superman                          306\n",
       "Batgirl                                         190\n",
       "All-American Men of War                         116\n",
       "100 Bullets                                     115\n",
       "The Adventures of Bob Hope                      109\n",
       "All-American Comics                             103\n",
       "All Star Western                                 97\n",
       "The Adventures of Jerry Lewis                    84\n",
       "80 Page Giant Magazine                           56\n",
       "52                                               52\n",
       "Advanced Dungeons & Dragons Comic Book           50\n",
       "The Adventures of Rex the Wonder Dog             46\n",
       "Action Comics Weekly                             42\n",
       "The Adventures of Dean Martin & Jerry Lewis      40\n",
       "Batman '66                                       31\n",
       "Adventures of the Outsiders                      28\n",
       "dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_value_counts(df, 'series_name')[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Speed investigates the murder of several Chinamen along the wharf area and uncovers a human smuggling operation. | A millionaire that has just bought the fabulously valuable Rhangwa Pearls receives a letter from someone threatening to steal them. | Confronted by a series of murders, Bret forages into the Peruvian jungles to ascertain why a hole appears in each victim's throat, but they were not shot! | Venturing into downtown San Francisco, Bruce finds it odd to see a Chinese restaurant among the buildings, and no one to wait upon him when he enters desiring a meal. Stranger still, another couple enters and is served anything they desire. When he decides to stick around and see what develops, he and the couple are suddenly grabbed and blindfolded! | Gus is sent to a rich woman's home to watch over her pearls during a party and nabs a man he sees pocketing them. Unfortunately for Gus, it is the Chief! | Bart is called on to volunteer for a secret spy case, but in doing so, he must forgo his own personal agenda, including his marriage to fiance Sally. | Jake is called in to solve the theft of some pearls, and promptly discovers that there were no pearls in the first place! | Series of four 1 panel detective-related gag cartoons. | Buck is called in to discover the truth behind some rustling charges between two cattle ranchers that have had hard feelings for each other since a land deal went bad. | The daughter of a chain-store magnate wants to hire Slam to guard her valuable poodle, and when he turns it down, Shorty volunteers. | Nelson is dragged off from the restaurant by a gang of Chinese and dumped outside of town. By the time he gets back, the entire restaurant has been cleaned out. His fellow diners have disappeared all together. | Slam and Shorty get caught in the middle of a double-cross. | Slam gets a job as a Hollywood stuntman. | In order to help out Slam's sixth grade teacher, Shorty has to go undercover as a student. | Bruce finds Sigrid in a house full of Chinese gangsters but she won't leave until they find her father. | Bart finally confesses to Sally why he tried to break off their relationship. After she helps him capture Olga Balinoff, she applies for a job with the same agency Bart works for. |  |  | An Asian man is inspecting jewels. | A police detective is hiding under a dock, with either a body or a suspect. | Bart and Sally are assigned to protect the steamship Atlantis. Sally pretends to be an anarchist in order to smoke the terrorist into the open. | Slam's new friend Danny dies in a boxing ring. Slam investigates Nick Cardoni for murder. | Man climbing up a rope about to be cut. | Rene D'Arnot is killed by Bart Regan. |  | A policeman is tied up next to a bomb with lit fuse. | Cosmo sees a man thrown out of a passing car. The body turns out to be Lou Capri, an East Side racketeer. | Shorty and Slam visit Stormhaven Alaska looking for Jack Trent. |  |  |  | Cosmo takes a vacation out west and gets involved in a case when his friend Sheriff Harvey is accused of murder. | Bart proposes to Sally | Slam and Shorty encounter a strange rocket shaped plane being flown by a girl. It turns out to be an invention that two scientists are fighting over. | A policeman watches a safecracker. | Dr. Wainwright's invisibility formula is stolen from his laboratory by the Raven, and assistant Adams is found tied up. Lured from the crime scene with word the Raven has been arrested, Kent is saved from killers by Wendy, whose father was swindled out of $4 million by the Raven. The real villain, disguised as Adams, kills Wainwright and uses his formula to temporarily become invisible. | Rosa, a female saboteur who has committed many acts of sabotage, interrupts the wedding of spies Bart and Sally, and uses a mirror that fires destructive rays. Unfortunately for Rosa, that leads to a spectacular, unfortunate end for her. | A policeman punches a criminal. | Two rival tongs fight over an ancient statue. | Renick is a show producer whose lead singer keeps dying. | Lee Travis, editor of the Globe Leader, dons a weird costume in order to track down a murderer. | Plot to steal a speedboat to use it to smuggle drugs. | A policeman is shot. | Slam Bradley disguises himself as Chinese in order to infiltrate an opium den, then pretends to smoke opium before making contact with an allied agent. | The Raven's henchmen gas the guards and help him escape from death row via a gyro-plane. Several weeks later, he attempts to steal the Duchess of Biltshire's diamonds worth $500,000. | While on vacation Saunders meets a girl who used to run with a gang. And there's a dead body. | Larry is mistakenly accused of murdering one of Lord Ashley's servants. | The Boss uses zombies to try to wipe out his underworld rivals. | At the homecoming game Bruce Nelson uncovers a gambling ring. | A murderer is disguising his work as heart attacks. | A man rings Cosmo's door in the middle of the night and claims someone wants to murder him. So Cosmo takes his place. | Professor Kenton tells Slam and Shorty about the fourth dimension and then takes them for a visit in his time-flier. | The Persian consul is charged with murder and Saunders has to prove his innocence. He uses his scientific detective skills to comb through the evidence. | Cosmo investigates sabotage at the Ace Foundry. Meanwhile Radsky agitates for a strike. | Bart and Sally search for gunrunners aboard a merchant ship. | The Crimson tries to break up an insurance racket. |  |  | In his first case, the Bat-Man investigates the murder of a chemical tycoon, discovering that one of his partners murdered him to steal the secret contracts that would leave him as the sole owner of the Apex Chemical Corporation. | A young boy scout discovers a lost youngster and attempts to help him find his way home, and gets lost in the process himself! | Speed investigates the murder of two men, on one of which the killers left the symbol of a red crescent, the symbol of the Kurdistan Killers, a sect that both men belonged to but refused to kill for. | Buck investigates the death of a ranch owner in which a black empty cartridge is left behind as a clue to the killer. | Bart investigates the deaths of members of a Committee on un-American activities that were killed by an explosive capsule hidden in their bananas. | The Crimson tracks down a killer who murdered one Abe Gold for welshing on a $100,000 bet. | Captain Byrne tracks down a revengeful killer who was once spurned in regards to a radio audtion. | Bruce begins his investigation of the death of a young woman due to black magic. | Fu Manchu attempts to kill Greba Eltham, all in an attempt to keep Rev. Eltham from going to China. | Flannigan is sent out on a bank robbery, but stops for a red light so he won't get a ticket! | Cosmo is placed on a case involving the illegal smuggling of Chinese into America. | Pete and Joe try to discover the whereabouts of the hideout of notorious criminal Louie th' Louse. | Shorty and Slam help to find out the whereabouts of a Federal agent who disappeared after trailing an escaped felon to Switzerland. | Using information obtained from a Police stool pigeon, the Bat-Man sets aim on one Frenchy Blake, leader of a notorious jewel theft ring. | Dr. Death plans to use his new invention of a poisonous pollen extract on any wealthy person who refuses to pay him tribute. | Thought dead in a fire in his home, Dr. Death has survived, but is in need of funds to re-establish himself, so he sets his sights on diamonds owned by a Mrs. Jones. | When Batman discovers Julie in a trance, he is advised by a doctor to take her to Hungary, where he faces a living vampire, who wishes to kill the Caped Crusader and feed Julie to his werewolves. | Reaching Hungary with Julie, the Batman runs into another vampire, who wants to make a deal with him: if she shows him the Monk's hideout, will he kill him? | Slam breaks up a hold-up by socking the wrong guy. | After an account o ...\n"
     ]
    }
   ],
   "source": [
    "def concat_values_by_series(df, concat_column, series_name):    \n",
    "\n",
    "    list_of_synopsis = df[\n",
    "        (df['series_name'] == series_name) & df['issue_number'] != 0.0].sort_values('issue_number')[concat_column]\n",
    "\n",
    "    return \" | \".join(list_of_synopsis.values)\n",
    "\n",
    "print(concat_values_by_series(df, concat_column='synopsis', series_name=\"Detective Comics\")[:8000], '...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dc US comics publisher page\n",
    "url = 'https://www.comics.org/publisher/54/?page=1'\n",
    "\n",
    "html = webscraper.simple_get(url)\n",
    "soup = webscraper.transform_simple_get_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bytes"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('../comicvision/resources/publisher_54_page_1', 'wb')\n",
    "# f.write(html)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse series' metadata\n",
    "\n",
    "name = soup.find_all('td', {'class': 'name'})\n",
    "year = soup.find_all('td', {'class': 'year'})\n",
    "issue_count = soup.find_all('td', {'class': 'issue_count'})\n",
    "covers = soup.find_all('td', {'class': 'covers'})\n",
    "published = soup.find_all('td', {'class': 'published'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: get list of series names and urls from a publisher page (e.g. page=1)\n",
    "\n",
    "n = [result.find('a').contents[0] for result in name]\n",
    "href = [result.find('a')['href'] for result in  name]\n",
    "y = [result.contents[0] for result in year]\n",
    "i = [result.contents[0] for result in issue_count]\n",
    "# c = [result.find('a').contents[0] for result in covers]\n",
    "p = [result.contents[0] for result in published]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: reason about a series' metadata (# of issues, #  of covers)\n",
    "\n",
    "# create series dataframe\n",
    "series_df = pd.DataFrame(list(zip(n, href,  y, i, p)), columns=['name', 'href', 'year', 'issue_count', 'published'])\n",
    "\n",
    "# parse issue count as int from issue_count  column\n",
    "series_df['issue_count_int'] = series_df['issue_count'].apply(lambda x: int(re.search(r'\\d+', x).group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>href</th>\n",
       "      <th>year</th>\n",
       "      <th>issue_count</th>\n",
       "      <th>published</th>\n",
       "      <th>issue_count_int</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>100 Bullets</td>\n",
       "      <td>/series/6133/</td>\n",
       "      <td>1999</td>\n",
       "      <td>100 issues (100 indexed)</td>\n",
       "      <td>August 1999 - April 2009</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>100 Bullets</td>\n",
       "      <td>/series/24535/</td>\n",
       "      <td>2000</td>\n",
       "      <td>13 issues (8 indexed)</td>\n",
       "      <td>[January] 2000 - [July] 2009</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1st Issue Special</td>\n",
       "      <td>/series/2212/</td>\n",
       "      <td>1975</td>\n",
       "      <td>13 issues (13 indexed)</td>\n",
       "      <td>April 1975 - April 1976</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>52</td>\n",
       "      <td>/series/16626/</td>\n",
       "      <td>2006</td>\n",
       "      <td>52 issues (52 indexed)</td>\n",
       "      <td>July 2006 - July 2007</td>\n",
       "      <td>52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>80 Page Giant Magazine</td>\n",
       "      <td>/series/1620/</td>\n",
       "      <td>1964</td>\n",
       "      <td>56 issues (15 indexed)</td>\n",
       "      <td>August 1964 - February-March 1969</td>\n",
       "      <td>56</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>Action Comics</td>\n",
       "      <td>/series/97/</td>\n",
       "      <td>1938</td>\n",
       "      <td>866 issues (866 indexed)</td>\n",
       "      <td>June 1938 - October 2011</td>\n",
       "      <td>866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>Action Comics</td>\n",
       "      <td>/series/59922/</td>\n",
       "      <td>2011</td>\n",
       "      <td>117 issues (117 indexed)</td>\n",
       "      <td>November 2011 - Present</td>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      name            href  year               issue_count  \\\n",
       "3              100 Bullets   /series/6133/  1999  100 issues (100 indexed)   \n",
       "4              100 Bullets  /series/24535/  2000     13 issues (8 indexed)   \n",
       "16       1st Issue Special   /series/2212/  1975    13 issues (13 indexed)   \n",
       "22                      52  /series/16626/  2006    52 issues (52 indexed)   \n",
       "34  80 Page Giant Magazine   /series/1620/  1964    56 issues (15 indexed)   \n",
       "98           Action Comics     /series/97/  1938  866 issues (866 indexed)   \n",
       "99           Action Comics  /series/59922/  2011  117 issues (117 indexed)   \n",
       "\n",
       "                            published  issue_count_int  \n",
       "3            August 1999 - April 2009              100  \n",
       "4        [January] 2000 - [July] 2009               13  \n",
       "16            April 1975 - April 1976               13  \n",
       "22              July 2006 - July 2007               52  \n",
       "34  August 1964 - February-March 1969               56  \n",
       "98           June 1938 - October 2011              866  \n",
       "99            November 2011 - Present              117  "
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_df[series_df['issue_count_int'] > 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://www.comics.org'\n",
    "series_urls = url + series_df['href']\n",
    "\n",
    "# take a series  with many issues...\n",
    "series_page_url = series_urls[98]\n",
    "\n",
    "series_page_html = webscraper.simple_get(series_page_url)\n",
    "series_page_soup = webscraper.transform_simple_get_html(series_page_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.comics.org/series/97/'"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "series_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('../comicvision/resources/series_97_page_1', 'wb')\n",
    "# f.write(html)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = 'https://www.comics.org'\n",
    "\n",
    "# get 'series details cover gallery' url\n",
    "cover_gallery_url = url + series_page_soup.find('a',  href=True, text='Cover Gallery')['href']\n",
    "cover_gallery_url = cover_gallery_url  + '/?page=22'\n",
    "\n",
    "cover_gallery_html = webscraper.simple_get(cover_gallery_url)\n",
    "cover_gallery_soup = webscraper.transform_simple_get_html(cover_gallery_html)\n",
    "\n",
    "if len(cover_gallery_soup.find_all('a', {'class': \"btn btn-default btn-sm\"})) > 0:\n",
    "\n",
    "    # get issue hrefs from all linked issues on cover gallery\n",
    "    cover_gallery_hrefs = filter(lambda x: '/issue/' in x['href'] and '/cover/' not in x['href'], cover_gallery_soup.find_all('a',  href=True))\n",
    "    issue_hrefs = [x['href'] for x in cover_gallery_hrefs]\n",
    "\n",
    "    #  construct issue urls from issue hrefs\n",
    "    issue_urls = [URL + issue_href for issue_href in  issue_hrefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.comics.org/series/97/covers/?page=22'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_gallery_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/issue/370657/',\n",
       " '/issue/370658/',\n",
       " '/issue/370659/',\n",
       " '/issue/370660/',\n",
       " '/issue/374477/',\n",
       " '/issue/374478/',\n",
       " '/issue/374479/',\n",
       " '/issue/374480/',\n",
       " '/issue/503361/',\n",
       " '/issue/512403/',\n",
       " '/issue/524135/',\n",
       " '/issue/524136/',\n",
       " '/issue/524137/',\n",
       " '/issue/524138/',\n",
       " '/issue/524139/',\n",
       " '/issue/524140/',\n",
       " '/issue/873707/',\n",
       " '/issue/524141/',\n",
       " '/issue/535174/',\n",
       " '/issue/545599/',\n",
       " '/issue/562837/',\n",
       " '/issue/562838/',\n",
       " '/issue/562839/',\n",
       " '/issue/606084/',\n",
       " '/issue/606085/',\n",
       " '/issue/606086/',\n",
       " '/issue/669657/',\n",
       " '/issue/669658/',\n",
       " '/issue/669659/',\n",
       " '/issue/682127/',\n",
       " '/issue/682128/',\n",
       " '/issue/702643/',\n",
       " '/issue/715100/',\n",
       " '/issue/732936/',\n",
       " '/issue/745730/',\n",
       " '/issue/750694/',\n",
       " '/issue/765369/']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_brackets(title: str) -> Union[str, None]:\n",
    "    \"\"\"\n",
    "    Return the substring of the first instance of bracketed text.\n",
    "    \"\"\"\n",
    "    regex_brackets = re.search(r\"\\[(.*?)\\]\", title)\n",
    "    if regex_brackets is None:\n",
    "        return None\n",
    "    else:\n",
    "        return regex_brackets.group()\n",
    "    \n",
    "\n",
    "def is_reprinting(title: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if a string contains some substrings.\n",
    "    \"\"\"\n",
    "    is_second_printing = \"Second Printing\" in title\n",
    "    is_2nd_printing = \"2nd Printing\" in title\n",
    "    is_3rd_printing = \"3rd Printing\" in title\n",
    "    is_4th_printing = \"4th Printing\" in title\n",
    "    is_5th_printing = \"5th Printing\" in title\n",
    "    is_6th_printing = \"6th Printing\" in title\n",
    "    is_7th_printing = \"7th Printing\" in title\n",
    "    is_8th_printing = \"8th Printing\" in title\n",
    "    is_9th_printing = \"9th Printing\" in title\n",
    "    is_10th_printing = \"10th Printing\" in title\n",
    "    return (\n",
    "        is_second_printing\n",
    "        | is_2nd_printing\n",
    "        | is_3rd_printing\n",
    "        | is_4th_printing\n",
    "        | is_5th_printing\n",
    "        | is_6th_printing\n",
    "        | is_7th_printing\n",
    "        | is_8th_printing\n",
    "        | is_9th_printing\n",
    "        | is_10th_printing\n",
    "    )\n",
    "\n",
    "\n",
    "def is_newsstand_or_canadian(title) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an issue is a duplicate Newsstand issue.\n",
    "    \"\"\"\n",
    "    return (\"newsstand\" in title.lower()) | (\"canadian\" in title.lower())\n",
    "\n",
    "\n",
    "def is_variant(title) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an issue is variant cover.\n",
    "    \"\"\"\n",
    "    return (\"variant\" in title.lower())\n",
    "\n",
    "\n",
    "def is_redundant(title: str) -> bool:\n",
    "    \"\"\"\n",
    "    Check if an issue is a redundant to a direct sale issue.\n",
    "    \"\"\"\n",
    "    if title is None:\n",
    "        return False\n",
    "    else:\n",
    "        return (is_reprinting(title) | is_newsstand_or_canadian(title) | is_variant(title)) | (('cover' in title.lower()) & ('direct' not in  title.lower()))\n",
    "\n",
    "\n",
    "cover_refs = [(x.get_text(), x['href']) for x in cover_gallery_soup.find_all('a',  href=True)]\n",
    "\n",
    "cover_refs = list(filter(lambda x: '/issue/' in x[1] and '/cover/' not in x[1], cover_refs))\n",
    "\n",
    "cover_refs = [(get_brackets(x[0]), x[1]) for x in cover_refs]\n",
    "\n",
    "cover_refs = list(filter(lambda x: not is_redundant(x[0]), cover_refs))\n",
    "\n",
    "issue_hrefs = [x[1] for x  in cover_refs]\n",
    "\n",
    "issue_urls = [URL + issue_href for issue_href in  issue_hrefs]\n",
    "\n",
    "# cover_refs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://www.comics.org/issue/370657/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "issue_urls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = open('../comicvision/resources/issue_370657', 'wb')\n",
    "# f.write(html)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take an issue url from the covers gallery..\n",
    "# issue_url = issue_urls[0]\n",
    "issue_url = \"https://www.comics.org/issue/21497/cover/4/\"\n",
    "\n",
    "issue_html = webscraper.simple_get(issue_url)\n",
    "issue_soup = webscraper.transform_simple_get_html(issue_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_soup.find_all(\"a\", {\"class\": \"btn btn-default btn-sm\"})\n",
    "\n",
    "# cover_gallery_pages = list(\n",
    "#                         filter(\n",
    "#                             lambda x: x.isdigit(),\n",
    "#                             [\n",
    "#                                 x.contents[0]\n",
    "#                                 for x in issue_soup.find_all(\n",
    "#                                     \"a\", {\"class\": \"btn btn-default btn-sm\"}\n",
    "#                                 )\n",
    "#                             ],\n",
    "#                         )\n",
    "#                     )\n",
    "\n",
    "# cover_gallery_range = max([int(x) for x in cover_gallery_pages])\n",
    "\n",
    "# cover_gallery_range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open('../comicvision/resources/issue_21497_cover_4', 'wb')\n",
    "f.write(issue_html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1].replace('/', '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_issue_metadata(soup, name):\n",
    "    if len(soup.find_all('dd', id=name)) > 0:\n",
    "        if (name != 'issue_indicia_publisher') & (name != 'issue_brand'):\n",
    "            return soup.find_all('dd', id=name)[0].contents[0].strip()\n",
    "        else:\n",
    "            return soup.find_all('dd', id=name)[0].find('a').contents[0]\n",
    "    else:\n",
    "        return \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get metadata from issue url\n",
    "\n",
    "# title, price, pages, color, dimension, paper_stock, binding, publishing_format\n",
    "metadata = {}\n",
    "\n",
    "metadata['title'] = issue_soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1].replace('/', '|')\n",
    "metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "\n",
    "metadata['on_sale_date'] = get_issue_metadata(issue_soup, name='on_sale_date')\n",
    "metadata['indicia_frequency'] = get_issue_metadata(issue_soup, name='indicia_frequency')\n",
    "metadata['issue_indicia_publisher'] = get_issue_metadata(issue_soup, name='issue_indicia_publisher')\n",
    "metadata['issue_brand'] = get_issue_metadata(issue_soup, name='issue_brand')\n",
    "metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "metadata['rating'] = get_issue_metadata(issue_soup, name='rating')\n",
    "metadata['indexer_notes'] = \" | \".join([x.contents[0].replace('\\n', '').strip() for x in issue_soup.find_all('p')])\n",
    "\n",
    "all_issue_credits = list(zip(\n",
    "    issue_soup.find_all('span', {'class': 'credit_label'}), \n",
    "    issue_soup.find_all('span', {'class': 'credit_value'})))\n",
    "\n",
    "metadata['synopsis'] = \" | \".join(list(filter(lambda x: x != '', [x[1].contents[0] if x[0].contents[0] == 'Synopsis' else '' for x in all_issue_credits])))\n",
    "\n",
    "\n",
    "# get cover section\n",
    "cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "# cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "cover_credits = list(zip(\n",
    "    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "))\n",
    "\n",
    "metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "metadata.pop('cover_reprints', None)\n",
    "\n",
    "# get the cover url\n",
    "cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "cover_img_url = URL + cover_img_href\n",
    "\n",
    "# get cover page\n",
    "cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "# get image urls from cover page\n",
    "cover_img_soup.find_all('img')\n",
    "\n",
    "cover_divs = cover_img_soup.find_all('div', {'class': 'issue_covers'})[0].find_all('div')\n",
    "\n",
    "def get_variant_cover_name(cover_name: str):\n",
    "    if get_brackets(cover_name) is None:\n",
    "        return 'Original Cover'\n",
    "    else:\n",
    "        return get_brackets(cover_name).replace('[','').replace(']','')\n",
    "\n",
    "# go into variant url and pull metadata\n",
    "cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "cover_names = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "covers_dict = {}\n",
    "for cover in covers:\n",
    "    name = cover[0]\n",
    "    url = cover[1]\n",
    "    image = cover[2]\n",
    "\n",
    "    covers_dict[name] = {}\n",
    "    covers_dict[name]['cover_url'] = url\n",
    "    covers_dict[name]['image_url'] = image\n",
    "\n",
    "metadata['variant_covers'] = {}\n",
    "    \n",
    "for variant_name in covers_dict:\n",
    "    if 'Second Printing' in variant_name:\n",
    "        pass\n",
    "    elif ('Newsstand' in variant_name) & ('Direct Sales' in covers_dict.keys()):\n",
    "        pass\n",
    "    else:\n",
    "        issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "        # get issue page\n",
    "        issue_html = webscraper.simple_get(issue_url)\n",
    "        issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "        cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "        cover_credits = list(zip(\n",
    "                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                            [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                        ))\n",
    "\n",
    "        cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "        cover_credits.pop('cover_reprints', None)\n",
    "        \n",
    "#         save_as = \"{} -- {} -- {} -- {}\".format(metadata['series_name'], strip_brackets_from_title(metadata['title']), variant_name, metadata['on_sale_date'], )\n",
    "#         save_to = './covers/' + save_as + '.jpg'\n",
    "        \n",
    "#         cover_credits['cover_image_file_name'] = save_as\n",
    "\n",
    "#         metadata['variant_covers'][variant_name] = cover_credits\n",
    "\n",
    "#         # save cover image\n",
    "#         urllib.request.urlretrieve(covers[variant_name], save_to)\n",
    "    \n",
    "    \n",
    "# # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "# # save metadata\n",
    "# with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "#     writer.write(metadata)\n",
    "\n",
    "# # TODO: write to log... timestamp/publisher/series/issue/\n",
    "# now = datetime.datetime.today().strftime(\"%d/%m/%Y %H:%M:%S\")\n",
    "# publisher_int = publisher_url.split('/')[-2]\n",
    "\n",
    "# log = {'timestamp': now, 'publisher': publisher_int, 'series': metadata['series_name'],  'issue': metadata['title']}\n",
    "# with jsonlines.open('./metadata/log.jsonl', mode='a') as writer:\n",
    "#     writer.write(log)\n",
    "\n",
    "# # slow down the requests so we don't take too many resources and get blocked\n",
    "# sleep(random.uniform(5, 10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.comics.org/issue/36858/\n",
      "https://www.comics.org/issue/1248505/\n",
      "https://www.comics.org/issue/1837315/\n"
     ]
    }
   ],
   "source": [
    "for variant_name in covers_dict:\n",
    "    print(covers_dict[variant_name]['cover_url'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "issue_url = \"https://www.comics.org/issue/1837315/\"\n",
    "\n",
    "issue_html = webscraper.simple_get(issue_url)\n",
    "issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "f = open('../comicvision/resources/issue_1837315', 'wb')\n",
    "f.write(issue_html)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1235,
   "metadata": {},
   "outputs": [],
   "source": [
    "# go into variant url and pull metadata\n",
    "cover_images = [x.find_all('a')[0].contents[0]['src'] for x in cover_divs]\n",
    "cover_names = [get_variant_cover_name(x.find_all('a')[1].contents[0]) for x in cover_divs]\n",
    "cover_urls = [URL + x.find_all('a')[0]['href'] for x in cover_divs]\n",
    "\n",
    "covers = list((zip(cover_names, cover_urls,  cover_images)))\n",
    "\n",
    "covers_dict = {}\n",
    "for cover in covers:\n",
    "    name = cover[0]\n",
    "    url = cover[1]\n",
    "    image = cover[2]\n",
    "\n",
    "    covers_dict[name] = {}\n",
    "    covers_dict[name]['cover_url'] = url\n",
    "    covers_dict[name]['image_url'] = image\n",
    "    \n",
    "    \n",
    "issue_url = covers_dict[variant_name]['cover_url']\n",
    "\n",
    "# get issue page\n",
    "issue_html = webscraper.simple_get(issue_url)\n",
    "issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "cover_credits = list(zip(\n",
    "                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                ))\n",
    "\n",
    "cover_credits = {\"cover_{}\".format(x[0].lower()): x[1] for x in cover_credits}\n",
    "cover_credits.pop('cover_reprints', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1256,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'cover_pencils': 'Gary Frank (signed)',\n",
       " 'cover_inks': 'Gary Frank (signed)',\n",
       " 'cover_colors': '?',\n",
       " 'cover_letters': 'typeset',\n",
       " 'cover_genre': 'superhero',\n",
       " 'cover_characters': 'Superman',\n",
       " 'cover_keywords': 'Legion of Super-Heroes flight ring'}"
      ]
     },
     "execution_count": 1256,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cover_credits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cover-vision (env)",
   "language": "python",
   "name": "cover-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
