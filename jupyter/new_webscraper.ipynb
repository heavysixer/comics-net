{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import jsonlines\n",
    "import pandas as pd\n",
    "import urllib.request\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.exceptions import RequestException\n",
    "\n",
    "import comicvision.webscraper as webscraper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: refactor this method to make it more functional and easier to test\n",
    "# TODO: add tests!\n",
    "\n",
    "# TODO: add random wait statement to mock obfuscate web scraping application from server\n",
    "# TODO: add log to track which publisher/series/issue has been completed\n",
    "\n",
    "def get_all_from_publisher_page(publisher_url: str, page: int):\n",
    "    \"\"\"\n",
    "    Do a thing...\n",
    "    \"\"\"\n",
    "    # global val\n",
    "    URL = 'https://www.comics.org'\n",
    "\n",
    "    # get publisher page\n",
    "    publisher_html = webscraper.simple_get(publisher_url + '?page={}'.format(page))\n",
    "    publisher_soup = webscraper.transform_simple_get_html(publisher_html)\n",
    "    \n",
    "    # parse series table from publisher page\n",
    "    series_name = [result.find('a').contents[0] for result in publisher_soup.find_all('td', {'class': 'name'})]\n",
    "    series_href = [result.find('a')['href'] for result in  publisher_soup.find_all('td', {'class': 'name'})]\n",
    "    series_year = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'year'})]\n",
    "    series_issue_count = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'issue_count'})]\n",
    "    # series_covers = [result.find('a').contents[0] for result in publisher_soup.find_all('td', {'class': 'covers'})]\n",
    "    series_published = [result.contents[0] for result in publisher_soup.find_all('td', {'class': 'published'})]\n",
    "    \n",
    "    # create dataframe of publisher series (on page)\n",
    "    series_df = pd.DataFrame(list(zip(series_name, series_href, series_year, series_issue_count, series_published)),\n",
    "                             columns=['name', 'href', 'year', 'issue_count', 'published'])\n",
    "\n",
    "    # parse issue count as int from issue_count column\n",
    "    series_df['issue_count_int'] = series_df['issue_count'].apply(lambda x: int(re.search(r'\\d+', x).group()))\n",
    "\n",
    "    # iterate over series dataframe and get issue covers and metadata\n",
    "    for series_name, series_page_href, issue_count in zip(series_df['name'], series_df['href'], series_df['issue_count_int']):\n",
    "        \n",
    "        if issue_count < 12:\n",
    "            pass\n",
    "        else:\n",
    "            print(series_name, series_page_href, issue_count)\n",
    "            \n",
    "            # construct series page url\n",
    "            series_page_url  = URL + series_page_href\n",
    "\n",
    "            # get series page\n",
    "            series_page_html = webscraper.simple_get(series_page_url)\n",
    "            series_page_soup = webscraper.transform_simple_get_html(series_page_html)\n",
    "\n",
    "            # get cover gallery url for series\n",
    "            cover_gallery_href = series_page_soup.find('a',  href=True, text='Cover Gallery')['href']\n",
    "            cover_gallery_url = URL + cover_gallery_href\n",
    "\n",
    "            # get cover gallery page\n",
    "            cover_gallery_html = webscraper.simple_get(cover_gallery_url)\n",
    "            cover_gallery_soup = webscraper.transform_simple_get_html(cover_gallery_html)\n",
    "\n",
    "            # get issue hrefs from all linked issues on cover gallery\n",
    "            issue_hrefs = list()\n",
    "            for i in range(1, (issue_count + 1)):\n",
    "                if cover_gallery_soup.find('a',  href=True,  text=i) is not None:\n",
    "                    issue_tag = cover_gallery_soup.find('a',  href=True,  text=i)\n",
    "                    issue_hrefs.append(issue_hrefs.append(issue_tag['href']))\n",
    "                    continue\n",
    "                elif cover_gallery_soup.find('a', href=True,  text='{} [Direct]'.format(i)) is not None:\n",
    "                    issue_tag = cover_gallery_soup.find('a', href=True,  text='{} [Direct]'.format(i))\n",
    "                    issue_hrefs.append(issue_hrefs.append(issue_tag['href']))\n",
    "                    continue\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            # filter out junk hrefs (I dunno... this is scraping... <shrug>)\n",
    "            issue_hrefs = list(filter(lambda x: 'issue' in x, [i for i in issue_hrefs if i]))\n",
    "\n",
    "            #  construct issue urls from issue hrefs\n",
    "            issue_urls = [URL + issue_href for issue_href in  issue_hrefs]\n",
    "\n",
    "            # scrape issues\n",
    "            for issue_url in issue_urls:\n",
    "\n",
    "                # get issue page\n",
    "                issue_html = webscraper.simple_get(issue_url)\n",
    "                issue_soup = webscraper.transform_simple_get_html(issue_html)\n",
    "\n",
    "                # metadata\n",
    "                metadata = {}\n",
    "                metadata['series_name'] = series_name.replace('/', '|')\n",
    "\n",
    "                # scrape metadata from issue page\n",
    "                # title, price, pages, color, dimension, paper_stock, binding, publishing_format\n",
    "                def get_issue_metadata(soup, name):\n",
    "                    if len(soup.find_all('dd', id=name)) > 0:\n",
    "                        return soup.find_all('dd', id=name)[0].contents[0].strip()\n",
    "                    else:\n",
    "                        return \"\"\n",
    "                \n",
    "                metadata['title'] = issue_soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1].replace('/', '|')\n",
    "                metadata['issue_price'] = get_issue_metadata(issue_soup, name='issue_price')\n",
    "                metadata['issue_pages'] = get_issue_metadata(issue_soup, name='issue_pages')\n",
    "                metadata['format_color'] = get_issue_metadata(issue_soup, name='format_color')\n",
    "                metadata['format_dimensions'] = get_issue_metadata(issue_soup, name='format_dimensions')\n",
    "                metadata['format_paper_stock'] = get_issue_metadata(issue_soup, name='format_paper_stock')\n",
    "                metadata['format_binding'] = get_issue_metadata(issue_soup, name='format_binding')\n",
    "                metadata['format_publishing_format'] = get_issue_metadata(issue_soup, name='format_publishing_format')\n",
    "\n",
    "                # get cover section and credits from issue page\n",
    "                cover = issue_soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "                # cover credits: editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "                cover_credits = list(zip(\n",
    "                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "                    [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "                ))\n",
    "\n",
    "                metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "                metadata.pop('cover_reprints', None)\n",
    "\n",
    "                # get the cover url\n",
    "                cover_img_href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "                cover_img_url = URL + cover_img_href\n",
    "\n",
    "                # get cover page\n",
    "                cover_img_html = webscraper.simple_get(cover_img_url)\n",
    "                cover_img_soup = webscraper.transform_simple_get_html(cover_img_html)\n",
    "\n",
    "                # get image urls from cover page\n",
    "                cover_images = cover_img_soup.select('img')\n",
    "\n",
    "                # get raw, highest res image\n",
    "                cover_images = list(filter(lambda x:'files1.comics.org//img/' in x['src'], cover_images))\n",
    "                cover = cover_images[0]['src']\n",
    "\n",
    "                # construct where to save the cover image\n",
    "                save_as = \"{} -- {}\".format(metadata['series_name'], metadata['title'])\n",
    "                save_to = './covers/' + save_as + '.jpg'\n",
    "                \n",
    "                metadata[\"cover_image_file_name\"] = save_as\n",
    "                \n",
    "                # save cover image\n",
    "                urllib.request.urlretrieve(cover, save_to)\n",
    "\n",
    "                # TODO: reason about response of save; if successful, save metadata, else contine\n",
    "                # save metadata\n",
    "                with jsonlines.open('./metadata/covers.jsonl', mode='a') as writer:\n",
    "                    writer.write(metadata)\n",
    "                    \n",
    "                # TODO: write to log... timestamp/publisher/series/issue/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Adventures of Spider-Man /series/17311/ 12\n",
      "Adventures of the Big Boy /series/11758/ 20\n",
      "The Adventures of the X-Men /series/11999/ 12\n",
      "The Adventures of the X-Men / The Adventures of Spider-Man /series/52528/ 12\n",
      "Age of Apocalypse /series/63931/ 14\n",
      "Agent X /series/11848/ 15\n",
      "Akira /series/3636/ 38\n",
      "ALF /series/3637/ 50\n",
      "Alias /series/10240/ 28\n",
      "Alien Legion /series/2858/ 20\n",
      "Alien Legion /series/3403/ 18\n",
      "The All New Exiles /series/9833/ 12\n",
      "All Surprise Comics /series/325/ 12\n",
      "All True Crime /series/626/ 16\n",
      "All-New Ghost Rider /series/79792/ 12\n",
      "All-New Guardians of the Galaxy /series/113811/ 12\n"
     ]
    }
   ],
   "source": [
    "get_all_from_publisher_page(publisher_url='https://www.comics.org/publisher/78/', page=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: load metadata and return aggregate / summary statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: write method to display cover image w/ cover metadata and add annotations to image\n",
    "# TODO: consider ideate/innotater for annotating directly in Jupyter notebooks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # marvel US comics publisher page\n",
    "# url = 'https://www.comics.org/publisher/78/?page=1'\n",
    "\n",
    "# html = webscraper.simple_get(url)\n",
    "# soup = webscraper.transform_simple_get_html(html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # parse series' metadata\n",
    "\n",
    "# name = soup.find_all('td', {'class': 'name'})\n",
    "# year = soup.find_all('td', {'class': 'year'})\n",
    "# issue_count = soup.find_all('td', {'class': 'issue_count'})\n",
    "# covers = soup.find_all('td', {'class': 'covers'})\n",
    "# published = soup.find_all('td', {'class': 'published'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: get list of series names and urls from a publisher page (e.g. page=1)\n",
    "\n",
    "# n = [result.find('a').contents[0] for result in name]\n",
    "# href = [result.find('a')['href'] for result in  name]\n",
    "# y = [result.contents[0] for result in year]\n",
    "# i = [result.contents[0] for result in issue_count]\n",
    "# # c = [result.find('a').contents[0] for result in covers]\n",
    "# p = [result.contents[0] for result in published]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TODO: reason about a series' metadata (# of issues, #  of covers)\n",
    "\n",
    "# # create series dataframe\n",
    "# series_df = pd.DataFrame(list(zip(n, href,  y, i, p)), columns=['name', 'href', 'year', 'issue_count', 'published'])\n",
    "\n",
    "# # parse issue count as int from issue_count  column\n",
    "# series_df['issue_count_int'] = series_df['issue_count'].apply(lambda x: int(re.search(r'\\d+', x).group()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# series_df[series_df['issue_count_int'] > 12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# url = 'https://www.comics.org'\n",
    "# series_urls = url + series_df['href']\n",
    "\n",
    "# # take a series  with many issues...\n",
    "# series_page_url = series_urls[6]\n",
    "\n",
    "# series_page_html = webscraper.simple_get(series_page_url)\n",
    "# series_page_soup = webscraper.transform_simple_get_html(series_page_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get 'series details cover gallery' url\n",
    "# cover_gallery_url = url + series_page_soup.find('a',  href=True, text='Cover Gallery')['href']\n",
    "\n",
    "# cover_gallery_html = webscraper.simple_get(cover_gallery_url)\n",
    "# cover_gallery_soup = webscraper.transform_simple_get_html(cover_gallery_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cover_gallery_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get issue hrefs from all linked issues on cover gallery\n",
    "# issue_hrefs = list()\n",
    "# for i in range(1, (84 + 1)):\n",
    "#     if cover_gallery_soup.find('a',  href=True,  text=i) is not None:\n",
    "#         issue_tag = cover_gallery_soup.find('a',  href=True,  text=i)\n",
    "#         issue_hrefs.append(issue_hrefs.append(issue_tag['href']))\n",
    "#         continue\n",
    "#     elif cover_gallery_soup.find('a', href=True,  text='{} [Direct]'.format(i)) is not None:\n",
    "#         issue_tag = cover_gallery_soup.find('a', href=True,  text='{} [Direct]'.format(i))\n",
    "#         issue_hrefs.append(issue_hrefs.append(issue_tag['href']))\n",
    "#         continue\n",
    "#     else:\n",
    "#         continue\n",
    "\n",
    "# # filter out junk hrefs (I dunno... this is scraping... <shrug>)\n",
    "# issue_hrefs = list(filter(lambda x: 'issue' in x, [i for i in issue_hrefs if i]))\n",
    "\n",
    "# #  construct issue urls from issue hrefs\n",
    "# issue_urls = [url + issue_href for issue_href in  issue_hrefs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # take an issue url from the covers gallery..\n",
    "# issue_url = issue_urls[0]\n",
    "\n",
    "# issue_html = webscraper.simple_get(issue_url)\n",
    "# issue_soup = webscraper.transform_simple_get_html(issue_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # get metadata from issue url\n",
    "\n",
    "# # title, price, pages, color, dimension, paper_stock, binding, publishing_format\n",
    "# metadata = {}\n",
    "\n",
    "# metadata['title'] = soup.find('title').contents[0].replace('\\n', '').strip().split(' :: ')[-1]\n",
    "# metadata['issue_price'] = soup.find_all('dd', id='issue_price')[0].contents[0].strip()\n",
    "# metadata['issue_pages'] = soup.find_all('dd', id='issue_pages')[0].contents[0].strip()\n",
    "# metadata['format_color'] = soup.find_all('dd', id='format_color')[0].contents[0].strip()\n",
    "# metadata['format_dimensions'] = soup.find_all('dd', id='format_dimensions')[0].contents[0].strip()\n",
    "# metadata['format_paper_stock'] = soup.find_all('dd', id='format_paper_stock')[0].contents[0].strip()\n",
    "# metadata['format_binding'] = soup.find_all('dd', id='format_binding')[0].contents[0].strip()\n",
    "# metadata['format_publishing_format'] = soup.find_all('dd', id='format_publishing_format')[0].contents[0].strip()\n",
    "\n",
    "\n",
    "# # get cover section\n",
    "# cover = soup.find(\"div\", {\"class\": \"cover\"})\n",
    "\n",
    "# # editing, script, pencils, inks, colors, letters, characters, etc...\n",
    "# cover_credits = list(zip(\n",
    "#     [result.contents[0] for result in cover.find_all('span', {'class': 'credit_label'})],\n",
    "#     [result.contents[0] for result in cover.find_all('span', {'class': 'credit_value'})]\n",
    "# ))\n",
    "\n",
    "# metadata.update({'cover_{}'.format(x.lower()): y for x, y in cover_credits})\n",
    "# metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# href = cover.find(\"div\", {'coverImage'}).a['href']\n",
    "# cover_img_url = 'https://www.comics.org' + href\n",
    "# cover_img_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw_html = webscraper.simple_get(cover_img_url)\n",
    "# html = BeautifulSoup(raw_html, 'html.parser')\n",
    "# images = html.select('img')\n",
    "\n",
    "# cover_images = list(filter(lambda x:'files1.comics.org//img/' in x['src'], images))\n",
    "\n",
    "# cover_image = cover_images[0]['src']\n",
    "# cover_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# issue_title = metadata['title']\n",
    "# issue_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save_to = './' + issue_title + '.jpg'\n",
    "\n",
    "# urllib.request.urlretrieve(cover_image, save_to)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cover-vision (env)",
   "language": "python",
   "name": "cover-vision"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
